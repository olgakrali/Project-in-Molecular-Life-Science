# Thursday 15 March, 2018

Today I am working the whole day on new dataset. I decided to take it a bit further and make my final train and test set larger.
Thus, my classifier has been trained on a dataset of 213 sequences (out of 1065 that my dataset contains) and tested on 89 sequences.
SVM takes the longest time, thus it ran around 3h for the binary-feature based SVM, and it runs quite sometime now for the PSSM input.
I would have finished so far, but I located some mistakes on my datasets that luckily I spotted otherwise I would have gotten the wrong results. But this is part of the learning process. In the past I would have spent days trying to spot whats is wrong, but I have learnt to write my scripts in a way that it is easy to go back and check every single step. Of course Print is my best friend. 

In the meantime, I ran the RandomForest and Decision Tree classifiers with class_weight = 'balanced' for both binary and PSSM feature input. It looks better than before, but nothing compares to the good predictive ability of my SVM model. 

In parallel, I wrote some of the README texts within the folders for a better guidance will exploring each file. New folders: Results and Final_predictor added and they wait for the final outputs. I am going to commit the largest models in compressed form, and I will do the same with parts of the PSSM folder for the final train and test set, since they exceed the GitHub memory limit. 
